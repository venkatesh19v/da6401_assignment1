{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA6401 - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and wandb setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WandB Log-in\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/venkatesh/Documents/IDL/Assignment_1/wandb/run-20250315_181834-jdrc7sq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/jdrc7sq3' target=\"_blank\">clear-brook-1107</a></strong> to <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/jdrc7sq3' target=\"_blank\">https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/jdrc7sq3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/jdrc7sq3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x76a89275f0e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the project in WandB\n",
    "projectId = \"DA6401_Assignment1\"\n",
    "wandb.init(project=projectId,\n",
    "    notes=\"This is Assignment 1 for the course DA6401\", \n",
    "    tags=[\"assignment_1\", \"fashion_mnist\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_test): \n",
    "    # Normalize the dataset (scale pixel values to [0, 1])\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "    # Reshape the data to add the channel dimension (28, 28, 1 for grayscale images)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-brook-1107</strong> at: <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/jdrc7sq3' target=\"_blank\">https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/jdrc7sq3</a><br> View project at: <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1</a><br>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250315_181834-jdrc7sq3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading fashion-MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "normalize_data(X_train, X_test)\n",
    "\n",
    "# Class names for f-MNIST dataset\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Dictionary to store one sample image for each class\n",
    "sample_images = {}\n",
    "\n",
    "# Iterating training data to collect one image per class\n",
    "for img, label in zip(X_train, y_train):\n",
    "    if label not in sample_images:\n",
    "        sample_images[label] = img\n",
    "    # Image for every class (0-9)\n",
    "    if len(sample_images) == 10:\n",
    "        break\n",
    "\n",
    "indexes = [0,1,3,5,6,8,16,18,19,23]\n",
    "# Log each image for the selected indexes\n",
    "for i in indexes:\n",
    "    img = X_train[i]\n",
    "    label = y_train[i]  # Use the label corresponding to the image\n",
    "    caption = class_names[label]  # Get the class name for the label\n",
    "    wandb.log({\"Fashion-MNIST images\": wandb.Image(img, caption=caption)})\n",
    "plt.tight_layout()\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # ReLU: f(x) = max(0,x) (0 to +ve infinite) \n",
    "    relu = np.maximum(0, x)\n",
    "    return relu\n",
    "\n",
    "def relu_derivative(x):\n",
    "    # ReLU Derivative: f(x)= {0 if x<0; 1 if x>0}  | (0. to 1.)\n",
    "    relu_derivative = (x > 0).astype(float) \n",
    "    return relu_derivative\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Sigmoid: f(x) = 1 / (1 + e^(-x)) | (0 to 1)\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # Sigmoid Derivative: f(x) = 1 / (1 + e^(-x)) * (1 - f(x)) | (0 to +ve infinite)\n",
    "    sig = sigmoid(x)\n",
    "    sigmoid_derivative = sig * (1 - sig)\n",
    "    return sigmoid_derivative\n",
    "\n",
    "def tanh(x):\n",
    "    # Tanh: f(x) = (e^x - e^(-x)) / (e^x + e^(-x)) | (-1 to 1)\n",
    "    tanh = np.tanh(x)\n",
    "    return tanh\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    # Tanh Derivative: f(x) = 1 - f(x)^2 | (0 to 1) \n",
    "    tanh_derivative = 1 - np.tanh(x)**2\n",
    "    return tanh_derivative\n",
    "\n",
    "def softmax(x):\n",
    "    # Softmax: f(x) = e^(x - max(x)) / sum(e^(x - max(x))) | (0 to 1)\n",
    "    ex_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    soft_max = ex_x / np.sum(ex_x, axis=1, keepdims=True)\n",
    "    return soft_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Neural Network class\n",
    "class FeedForwardNN:\n",
    "    # Initializing Constructor\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation='relu', weight_init='random'):\n",
    "        self.activation_name = activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_derivative = tanh_derivative\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "        \n",
    "        # Layer sizes and Initialize parameters\n",
    "        self.layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            if weight_init == \"random\":\n",
    "                W = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * 0.01\n",
    "            elif weight_init == \"xavier\":\n",
    "                W = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1.0 / self.layer_sizes[i])\n",
    "            else:\n",
    "                print(f\"Invalid weight_init: {weight_init}\")\n",
    "                raise ValueError(\"Unsupported weight initialization method.\")\n",
    "\n",
    "            b = np.zeros((1, self.layer_sizes[i+1]))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass and caches intermediate activations and linear transforms.\n",
    "        Returns the output and a cache dictionary.\n",
    "        Pre-activation values (Z): \n",
    "        (a -> Z, h -> A)\n",
    "        Linear transformation at each layer : ai(x)=Wi hi-1(x)+bi\n",
    "\n",
    "        Activations (A): \n",
    "        After applying the activation function : hi(x) = g(ai(x)) \n",
    "        \"\"\"\n",
    "        cache = {\"A\": [], \"Z\": []}\n",
    "        A = X\n",
    "        cache[\"A\"].append(A)  # A0 = input\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            \"\"\"\n",
    "            Z[i]=A[i-1]W[i]+b[i]\n",
    "            Z[i] is the linear transformation (input to activation) at layer i,\n",
    "            A[i-1] is the output of the previous layer,\n",
    "            W and be are weights and biases for layer i.\n",
    "            \"\"\"\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            cache[\"Z\"].append(Z)\n",
    "            A = self.activation(Z)\n",
    "            cache[\"A\"].append(A)\n",
    "        \n",
    "        # Output layer with softmax activation function\n",
    "        Z = np.dot(A, self.weights[-1]) + self.biases[-1]\n",
    "        cache[\"Z\"].append(Z)\n",
    "        A = softmax(Z)\n",
    "        cache[\"A\"].append(A)\n",
    "        return A, cache\n",
    "\n",
    "    def cross_entropy_loss(self, Y_pred, Y_true):\n",
    "        \"\"\"\n",
    "        Computes cross-entropy loss.\n",
    "        \n",
    "        m is the number of examples,\n",
    "        Y_true is the one-hot encoded vector of true labels for the i-th example,\n",
    "        Y_pred is the predicted probability distribution of the i-th example.\n",
    "        \"\"\"\n",
    "        m = Y_true.shape[0]\n",
    "        loss = -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m\n",
    "        return loss\n",
    "    \n",
    "    def compute_accuracy(self, Y_pred, Y_true):\n",
    "        # Computes the accuracy of the model.\n",
    "\n",
    "        predictions = np.argmax(Y_pred, axis=1)\n",
    "        labels = np.argmax(Y_true, axis=1)\n",
    "        return np.mean(predictions == labels)\n",
    "\n",
    "\n",
    "    def backward(self, X, Y_true, cache):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to compute gradients for weights and biases.\n",
    "        Returns lists of gradients for weights and biases.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        L = len(self.weights)  # Total number of layers\n",
    "        grads_W = [None] * L\n",
    "        grads_b = [None] * L\n",
    "\n",
    "        # Output layer gradient\n",
    "        A_final = cache[\"A\"][-1]\n",
    "        dZ = A_final - Y_true  # derivative for softmax + cross entropy (output layer)\n",
    "        grads_W[L - 1] = np.dot(cache[\"A\"][-2].T, dZ) / m # dW = A_prev * dZ/m (gradient of weights)\n",
    "        grads_b[L - 1] = np.sum(dZ, axis=0, keepdims=True) / m # db = dZ/m (gradient of biases)\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(L-2, -1, -1):\n",
    "            dA = np.dot(dZ, self.weights[i + 1].T)\n",
    "            Z = cache[\"Z\"][i]\n",
    "            dZ = dA * self.activation_derivative(Z)\n",
    "            grads_W[i] = np.dot(cache[\"A\"][i].T, dZ) / m\n",
    "            grads_b[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "        return grads_W, grads_b\n",
    "\n",
    "    def update_parameters(self, grads_W, grads_b, optimizer):\n",
    "        \"\"\"\n",
    "        Combines weights and biases into a single list, updates them using the provided optimizer,\n",
    "        and then splits them back into weights and biases.\n",
    "\n",
    "        W[i]=W[i]-η⋅∂Loss/∂W[i]\n",
    "        \"\"\"\n",
    "        # Combine parameters and gradients into one list each.\n",
    "        params = self.weights + self.biases\n",
    "        grads = grads_W + grads_b\n",
    "\n",
    "        # Use the optimizer to update the parameters in place.\n",
    "        updated_params = optimizer.update(params, grads)\n",
    "\n",
    "        # Split updated_params back into weights and biases.\n",
    "        L = len(self.weights)\n",
    "        self.weights = updated_params[:L]\n",
    "        self.biases = updated_params[L:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    Base Optimizer class that outlines the interface for optimization algorithms.\n",
    "    params (list): List of model parameters (weights and biases).\n",
    "    grads (list): List of gradients with respect to the model parameters.\n",
    "    \"\"\"\n",
    "    def update(self, params, grads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD)\n",
    "    Formula:\n",
    "        θ = θ - η * ∇θ\n",
    "    where, \n",
    "        θ represents the model parameters (weights/biases),\n",
    "        η is the learning rate, and \n",
    "        ∇θ is the gradient of the loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "        return params\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "    \"\"\"\n",
    "    Momentum optimizer class that helps accelerate SGD by considering past (history) gradients.\n",
    "    Formula:\n",
    "        v_t = β * v_(t-1) + ∇wt\n",
    "    where,\n",
    "        v_t is the velocity, \n",
    "        β is the momentum term, and \n",
    "        η is the learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]\n",
    "        return params\n",
    "\n",
    "class NAGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Nesterov Accelerated Gradient (NAG), improved version of momentum.\n",
    "    Formula:\n",
    "        v_t = β * v_(t-1) + ∇(wt - β * v_(t-1))   \n",
    "    where, \n",
    "        v_t is the momentum term, \n",
    "        β is the momentum coefficient, \n",
    "        η is the learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            v_prev = self.v[i].copy()\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            # Nesterov update: use the gradient evaluated at the lookahead position\n",
    "            params[i] += -self.momentum * v_prev + (1 + self.momentum) * self.v[i]\n",
    "        return params\n",
    "\n",
    "class RMSProp(Optimizer):\n",
    "    \"\"\"\n",
    "    RMSProp - Root Mean Square Propagation, which adjusts the learning rate based on recent gradients' magnitude.\n",
    "    Formula:\n",
    "        S_t = β * S_(t-1) + (1 - β) * (∇wt)^2\n",
    "        wt+1 = wt - η * ∇wt / sqrt(S_t + ε)\n",
    "    where, \n",
    "        S_t is the moving average of squared gradients, \n",
    "        β is the decay term,\n",
    "        η is the learning rate, and     \n",
    "        ε is a small number to prevent division by zero.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.S = None\n",
    "    def update(self, params, grads):\n",
    "        if self.S is None:\n",
    "            self.S = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            self.S[i] = self.beta * self.S[i] + (1 - self.beta) * (grads[i] ** 2)\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.S[i]) + self.epsilon)\n",
    "        return params\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    Adam optimizer, which combines momentum and RMSProp to adapt learning rates for each parameter.\n",
    "\n",
    "    Formula:\n",
    "        m_t = β1 * m_(t-1) + (1 - β1) * ∇wt\n",
    "        v_t = β2 * v_(t-1) + (1 - β2) * (∇wt))^2\n",
    "        m_t_hat = m_t / (1 - β1^t)\n",
    "        v_t_hat = v_t / (1 - β2^t)\n",
    "        wt = wt - η * m_t_hat / (sqrt(v_t_hat) + ε)\n",
    "    where,\n",
    "        m_t and v_t are moment estimates, \n",
    "        β1 and β2 are exponential decay rates, and \n",
    "        t is the time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        self.t += 1\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads[i] ** 2)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            params[i] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return params\n",
    "\n",
    "class Nadam(Optimizer):\n",
    "    \"\"\"\n",
    "    Nadam (Nesterov-accelerated Adaptive Moment Estimation),\n",
    "\n",
    "    Formula:\n",
    "        m_t+1 = β1 * m_t + (1 - β1) * ∇wt\n",
    "        v_t+1 = β2 * v_t + (1 - β2) * (∇wt)^2\n",
    "        m_t_hat = m_t+1 / (1 - β1^t+1)\n",
    "        v_t_hat = v_t+1 / (1 - β2^t+1)\n",
    "        wt+1 = wt - (η / (sqrt(v_t_hat) + ε)* (β1 * m_t+1_hat + (1 - β1) * ∇wt/ 1 - β1_t+1)\n",
    "    where, \n",
    "        β1 and β2 are exponential decay rates, and\n",
    "        t is the time step.\n",
    "    \"\"\"    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        self.t += 1\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads[i] ** 2)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            # Nadam uses a Nesterov-like momentum term:\n",
    "            nesterov_m = (self.beta1 * m_hat) + ((1 - self.beta1) * grads[i] / (1 - self.beta1 ** self.t))\n",
    "            params[i] -= self.lr * nesterov_m / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Initializing a matrix of zeros with shape (number of samples, number of classes)\n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_optimizer(name):\n",
    "    if name == \"sgd\":\n",
    "        return SGD(learning_rate=wandb.config.learning_rate)\n",
    "    elif name == \"momentum\":\n",
    "        momentum = wandb.config.get('momentum', 0.9)  # Default to 0.9 if not in config\n",
    "        return Momentum(learning_rate=wandb.config.learning_rate, momentum=momentum)\n",
    "    elif name == \"nagd\":\n",
    "        momentum = wandb.config.get('momentum', 0.9)  # Default to 0.9 if not in config\n",
    "        return NAGD(learning_rate=wandb.config.learning_rate, momentum=momentum)\n",
    "    elif name == \"rmsprop\":\n",
    "        beta = wandb.config.get('beta', 0.9)  # Default to 0.9 if not in config\n",
    "        epsilon = wandb.config.get('epsilon', 1e-8)  # Default to 1e-8 if not in config\n",
    "        return RMSProp(learning_rate=wandb.config.learning_rate, beta=beta, epsilon=epsilon)\n",
    "    elif name == \"adam\":\n",
    "        beta1 = wandb.config.get('beta1', 0.9)  # Default to 0.9 if not in config\n",
    "        beta2 = wandb.config.get('beta2', 0.999)  # Default to 0.999 if not in config\n",
    "        epsilon = wandb.config.get('epsilon', 1e-8)  # Default to 1e-8 if not i\n",
    "        return Adam(learning_rate=wandb.config.learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "    elif name == \"nadam\":\n",
    "        beta1 = wandb.config.get('beta1', 0.9)  # Default to 0.9 if not in config\n",
    "        beta2 = wandb.config.get('beta2', 0.999)  # Default to 0.999 if not in config\n",
    "        epsilon = wandb.config.get('epsilon', 1e-8)  # Default to 1e-8 if not i\n",
    "        return Nadam(learning_rate=wandb.config.learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented a feedforward neural network which takes images from the fashion-mnist data as input and  \n",
    "\n",
    "outputs a probability distribution over the 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, X_train, Y_train, epochs, batch_size):\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the training data to ensure randomness and avoid overfitting\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X_train[permutation]\n",
    "        Y_shuffled = Y_train[permutation]\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            Y_batch = Y_shuffled[start:end]\n",
    "            Y_pred, cache = model.forward(X_batch)\n",
    "            loss = model.cross_entropy_loss(Y_pred, Y_batch)\n",
    "            accuracy = model.compute_accuracy(Y_pred, Y_batch)\n",
    "            epoch_loss += loss\n",
    "            grads_W, grads_b = model.backward(X_batch, Y_batch, cache)\n",
    "            model.update_parameters(grads_W, grads_b, optimizer)  # Updated model parameters (weights and biases) using the optimizer\n",
    "\n",
    "        # Calculate the average loss for the epoch (mean loss over all batches)\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy}\")\n",
    "        wandb.log({\"epoch\": epoch+1, \"loss\": avg_loss, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/venkatesh/Documents/IDL/Assignment_1/wandb/run-20250310_113852-aa16ilxg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/aa16ilxg' target=\"_blank\">snowy-bird-699</a></strong> to <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/aa16ilxg' target=\"_blank\">https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/aa16ilxg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.9551, Accuracy: 0.75\n",
      "Epoch 2/5, Loss: 0.6106, Accuracy: 0.75\n",
      "Epoch 3/5, Loss: 0.5030, Accuracy: 0.78125\n",
      "Epoch 4/5, Loss: 0.4184, Accuracy: 0.875\n",
      "Epoch 5/5, Loss: 0.3748, Accuracy: 0.875\n",
      "Probability distribution over 10 classes of a single sample: [3.40056662e-12 5.98004125e-20 5.95066211e-09 1.04029727e-12\n",
      " 3.79696957e-12 4.87327255e-05 6.05602626e-14 3.89352547e-03\n",
      " 2.15995508e-05 9.96036136e-01]\n",
      "Probability distribution over 10 classes of a single sample: [9.92001570e-01 4.94621715e-10 3.29042845e-04 7.64097396e-07\n",
      " 2.80373712e-10 1.20574562e-20 7.66861598e-03 1.35607295e-31\n",
      " 5.80317478e-09 5.80942578e-19]\n",
      "Probability distribution over 10 classes of a single sample: [2.60643985e-01 4.77829907e-02 8.45275302e-02 1.85053355e-01\n",
      " 2.57978709e-02 2.33953407e-06 3.95772063e-01 5.46036393e-11\n",
      " 4.19865654e-04 1.99961327e-10]\n",
      "Probability distribution over 10 classes of a single sample: [2.63661974e-01 1.80681259e-03 1.29043908e-02 2.91787768e-01\n",
      " 6.23265131e-03 5.72541036e-09 4.20853252e-01 4.10805778e-13\n",
      " 2.75314580e-03 8.48161359e-12]\n",
      "Probability distribution over 10 classes of a single sample: [1.92659081e-01 4.41162516e-02 3.98701102e-03 6.91082468e-01\n",
      " 2.24007338e-03 5.28509985e-08 6.58982699e-02 5.57597336e-15\n",
      " 1.67918879e-05 1.66645856e-13]\n",
      "Predictions for first 5 training samples: [9 0 6 6 3]\n",
      "Original labels for test data: [9 2 1 ... 8 1 5]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▃██</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.875</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>0.37475</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">snowy-bird-699</strong> at: <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/aa16ilxg' target=\"_blank\">https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1/runs/aa16ilxg</a><br> View project at: <a href='https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">https://wandb.ai/venkatesh19v-indian-institute-of-technology-madras/DA6401_Assignment1</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250310_113852-aa16ilxg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    wandb.init(\n",
    "        project=projectId, \n",
    "        config={\n",
    "            \"input_size\": 784,\n",
    "            \"hidden_layers\": [128, 64, 32],\n",
    "            \"output_size\": 10,\n",
    "            \"activation\": \"relu\",     \n",
    "            \"weight_init\": \"random\",    \n",
    "            \"learning_rate\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"beta\": 0.9,               \n",
    "            \"beta1\": 0.9,              \n",
    "            \"beta2\": 0.999,            \n",
    "            \"epsilon\": 1e-8,\n",
    "            \"optimizer\": \"adam\",   \n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 5\n",
    "        })\n",
    "    \n",
    "    config = wandb.config\n",
    "    samples = 5\n",
    "\n",
    "    # Load and preprocessing Fashion-MNIST data\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "    Y_train = one_hot_encode(y_train, config.output_size)\n",
    "    Y_test = one_hot_encode(y_test, config.output_size)\n",
    "\n",
    "    # Initializing model and optimizer\n",
    "    model = FeedForwardNN(config.input_size, config.hidden_layers, config.output_size,\n",
    "                          activation=config.activation, weight_init=config.weight_init)\n",
    "    optimizer = get_optimizer(config.optimizer)\n",
    "\n",
    "    # Training the model\n",
    "    train(model, optimizer, X_train, Y_train, epochs=config.epochs, batch_size=config.batch_size)\n",
    "\n",
    "    # After training, evaluating on a few samples and log results\n",
    "    Y_pred, _ = model.forward(X_train[:samples])\n",
    "    for i in range(Y_pred[:samples].shape[0]):\n",
    "        print(\"Probability distribution over 10 classes of a single sample:\", Y_pred[i])\n",
    "    predictions = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "    test_set = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    print(\"Predictions for first 5 training samples:\", predictions)\n",
    "    print(\"Original labels for test data:\", test_set)\n",
    "    \n",
    "    # Loging table of predictions\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    table = wandb.Table(columns=[\"Sample\", \"Predicted Class\", \"Class Name\"])\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        table.add_data(i, pred, class_names[pred])\n",
    "    wandb.log({\"Predictions\": table})\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the predicted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x900 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To verify the predictions for samples\n",
    "true_classes = np.argmax(Y_train[:samples], axis=1)  # True class indices\n",
    "\n",
    "fig, axes = plt.subplots(1, samples, figsize=(15, 9))  # 1 rows, 5 columns\n",
    "axes = axes.flatten()  # Flatten axes array for easy indexing\n",
    "\n",
    "for i in range(samples):\n",
    "    # Plot the image\n",
    "    ax = axes[i]\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap=\"gray\")  # Reshaping the flattened image to 28x28\n",
    "\n",
    "    # Title showing prediction result\n",
    "    pred_class = predictions[i]\n",
    "    true_class = true_classes[i]\n",
    "    pred_class_name = class_names[pred_class]\n",
    "    true_class_name = class_names[true_class]\n",
    "    \n",
    "    # Check if the prediction is correct or not\n",
    "    correct = 'Correct' if pred_class == true_class else 'Incorrect'\n",
    "\n",
    "    ax.set_title(f\"Pred: {pred_class_name}\\nTrue: {true_class_name}\\n{correct}\")\n",
    "    ax.axis('off')  # Hide axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "table = wandb.Table(columns=[\"Sample\", \"Predicted Class\", \"True Class\", \"Correct?\"])\n",
    "for i in range(samples):\n",
    "    pred_class = predictions[i]\n",
    "    true_class = true_classes[i]\n",
    "    pred_class_name = class_names[pred_class]\n",
    "    true_class_name = class_names[true_class]\n",
    "    correct = 'Correct' if pred_class == true_class else 'Incorrect'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split of fashion_mnist and 10% of the training data aside as validation data for this hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    # Initialize wandb for the current run. The hyperparameters come from wandb.config.\n",
    "    wandb.init()\n",
    "\n",
    "    config = wandb.config\n",
    "    run_name = f\"hl_{config.hidden_layers}_hs_{config.hidden_size}_ep_{config.epochs}_bs_{config.batch_size}_lr_{config.learning_rate}_wi_{config.weight_init}_wd_{config.weight_decay}_op_{config.optimizer}_ac_{config.activation.lower()}\"\n",
    "    print(run_name)\n",
    "\n",
    "    wandb.run.name = run_name\n",
    "    input_size = config.input_size\n",
    "    output_size = config.output_size\n",
    "    hidden_layers = config.hidden_layers  # Number of hidden layers\n",
    "    hidden_size = config.hidden_size  # Size of each hidden layer\n",
    "    activation_function = config.activation  # Activation function\n",
    "    weight_init = config.weight_init  # Weight initialization method\n",
    "    optimizer_name = config.optimizer  # Optimizer choice\n",
    "    batch_size = config.batch_size  # Batch size\n",
    "    num_epochs = config.epochs  # Number of epochs\n",
    "    learning_rate = config.learning_rate  # Learning rate\n",
    "    weight_decay = config.weight_decay  # Weight decay (L2 regularization)\n",
    "\n",
    "\n",
    "\n",
    "    # Load Fashion-MNIST data (using the standard train/test split)\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "    # Preprocess the images: flatten and normalize to [0, 1]\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "    # Create a validation split: 90% training, 10% validation\n",
    "    num_train = int(0.9 * X_train.shape[0])\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "    X_train = X_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "    X_train_split = X_train[:num_train]\n",
    "    X_val_split = X_train[num_train:]\n",
    "    y_train_split = y_train[:num_train]\n",
    "    y_val_split = y_train[num_train:]\n",
    "\n",
    "    # Use the local variable \"output_size\" instead of config.output_size\n",
    "    num_classes = output_size  # typically 10\n",
    "    Y_train_split = one_hot_encode(y_train_split, num_classes)\n",
    "    Y_val_split = one_hot_encode(y_val_split, num_classes)\n",
    "    hidden_layers = [hidden_size] * hidden_layers  # This will create a list of hidden layers\n",
    "\n",
    "    # Instantiate the model (FeedForwardNN is assumed to be defined with backprop as in Question 3)\n",
    "    model = FeedForwardNN(\n",
    "        input_size=input_size,\n",
    "        output_size=output_size,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation=activation_function.lower(),  # convert to lowercase if needed\n",
    "        weight_init=weight_init.lower()\n",
    "    )\n",
    "    optimizer = get_optimizer(optimizer_name)\n",
    "    num_samples = X_train_split.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data at each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_train_epoch = X_train_split[permutation]\n",
    "        Y_train_epoch = Y_train_split[permutation]\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_train_epoch[start:end]\n",
    "            Y_batch = Y_train_epoch[start:end]\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred, cache = model.forward(X_batch)\n",
    "            loss = model.cross_entropy_loss(Y_pred, Y_batch)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            grads_W, grads_b = model.backward(X_batch, Y_batch, cache)\n",
    "\n",
    "            # Update parameters using the chosen optimizer\n",
    "            model.update_parameters(grads_W, grads_b, optimizer)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches   # training_loss = avg_loss\n",
    "\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        Y_val_pred, _ = model.forward(X_val_split)\n",
    "        val_loss = model.cross_entropy_loss(Y_val_pred, Y_val_split)\n",
    "\n",
    "        predictions = np.argmax(Y_val_pred, axis=1)\n",
    "        correct = np.sum(predictions == y_val_split)\n",
    "        val_accuracy_epoch = correct / len(y_val_split)\n",
    "\n",
    "\n",
    "        # Log epoch-level metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"loss\": avg_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy_epoch,\n",
    "        })\n",
    "\n",
    "    print(f\"Sweep run complete: Best Epoch: {best_epoch}, Best Val Loss: {best_val_loss:.4f}, Val Accuracy: {val_accuracy_epoch:.4f}\")\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  #(bayes) Using random search for efficiency in high-dimensional spaces\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'name': 'hyperparams_sweeping',\n",
    "    'parameters': {\n",
    "        'epochs': {'values': [5, 10, 15, 20]},\n",
    "        'hidden_layers': {'values': [3, 4, 5, 6, 7]},  # Number of hidden layers\n",
    "        'hidden_size': {'values': [32, 64, 128, 256]},  # Size of each hidden layer\n",
    "        'weight_decay': {'values': [0, 0.0005, 0.5]},  # L2 regularization factor\n",
    "        'learning_rate': {'values': [0.0001, 0.001, 0.01, 0.1]},\n",
    "        'optimizer': {'values': ['sgd', 'momentum', 'nagd', 'rmsprop', 'adam', 'nadam']},\n",
    "        'batch_size': {'values': [8, 16, 32, 64]},\n",
    "        'weight_init': {'values': ['random', 'xavier']},\n",
    "        'activation': {'values': ['sigmoid', 'tanh', 'relu']},  # Use lowercase for consistency\n",
    "        'input_size': {'value': 784},  # Fixed input size for Fashion-MNIST\n",
    "        'output_size': {'value': 10},   # Fixed output classes\n",
    "        'epsilon':{ 'values': [1e-8, 1e-7, 1e-6]},\n",
    "        'beta': {'values': [0.9, 0.99, 0.999]},  # Possible values for beta\n",
    "        'momentum': {'values': [0.8, 0.9, 0.95]},  # Possible values for momentum\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project=projectId)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wandb.agent(sweep_id, function=train_sweep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
