{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Log-in\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the project in WandB\n",
    "projectId = \"temp-project\"\n",
    "wandb.init(project=projectId,\n",
    "    notes=\"This is Assignment 1 for the course DA6401\", \n",
    "    tags=[\"assignment_1\", \"fashion_mnist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading fashion-MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the dataset (scale pixel values to [0, 1])\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data to add the channel dimension (28, 28, 1 for grayscale images)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Class names for f-MNIST dataset\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Dictionary to store one sample image for each class\n",
    "sample_images = {}\n",
    "\n",
    "# Iterating training data to collect one image per class\n",
    "for img, label in zip(X_train, y_train):\n",
    "    if label not in sample_images:\n",
    "        sample_images[label] = img\n",
    "    # Image for every class (0-9)\n",
    "    if len(sample_images) == 10:\n",
    "        break\n",
    "\n",
    "# Create a grid plot (2 rows x 5 columns) to display the images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "table = wandb.Table(columns=[\"Image\", \"Label\"])\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(sample_images[i], cmap='gray')\n",
    "    axes[i].set_title(f\"Class {i}: {class_names[i]}\")\n",
    "    axes[i].axis('off')\n",
    "    img = sample_images[i] * 255  # Rescale for visualization\n",
    "    label = class_names[i]  # Get class name\n",
    "    table.add_data(wandb.Image(img), label)  # Add to table\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Log the plot to wandb\n",
    "wandb.log({\"fashion_mnist_samples\": table})\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # ReLU: f(x) = max(0,x) (0 to +ve infinite) \n",
    "    relu = np.maximum(0, x)\n",
    "    return relu\n",
    "\n",
    "def relu_derivative(x):\n",
    "    # ReLU Derivative: f(x)= {0 if x<0; 1 if x>0}  | (0. to 1.)\n",
    "    relu_derivative = (x > 0).astype(float) \n",
    "    return relu_derivative\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Sigmoid: f(x) = 1 / (1 + e^(-x)) | (0 to 1)\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    # Sigmoid Derivative: f(x) = 1 / (1 + e^(-x)) * (1 - f(x)) | (0 to +ve infinite)\n",
    "    sig = sigmoid(x)\n",
    "    sigmoid_derivative = sig * (1 - sig)\n",
    "    return sigmoid_derivative\n",
    "\n",
    "def tanh(x):\n",
    "    # Tanh: f(x) = (e^x - e^(-x)) / (e^x + e^(-x)) | (-1 to 1)\n",
    "    tanh = np.tanh(x)\n",
    "    return tanh\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    # Tanh Derivative: f(x) = 1 - f(x)^2 | (0 to 1) \n",
    "    tanh_derivative = 1 - np.tanh(x)**2\n",
    "    return tanh_derivative\n",
    "\n",
    "def softmax(x):\n",
    "    # Softmax: f(x) = e^(x - max(x)) / sum(e^(x - max(x))) | (0 to 1)\n",
    "    ex_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    soft_max = ex_x / np.sum(ex_x, axis=1, keepdims=True)\n",
    "    return soft_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForward Neural Network class\n",
    "class FeedForwardNN:\n",
    "    # Initializing Constructor\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation='relu', weight_init='random'):\n",
    "        self.activation_name = activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_derivative = tanh_derivative\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "        \n",
    "        # Layer sizes and Initialize parameters\n",
    "        self.layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            if weight_init == \"random\":\n",
    "                W = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * 0.01\n",
    "            elif weight_init == \"xavier\":\n",
    "                W = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * np.sqrt(1.0 / self.layer_sizes[i])\n",
    "            else:\n",
    "                print(f\"Invalid weight_init: {weight_init}\")\n",
    "                raise ValueError(\"Unsupported weight initialization method.\")\n",
    "\n",
    "            b = np.zeros((1, self.layer_sizes[i+1]))\n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs a forward pass and caches intermediate activations and linear transforms.\n",
    "        Returns the output and a cache dictionary.\n",
    "        Pre-activation values (Z): \n",
    "        (a -> Z, h -> A)\n",
    "        Linear transformation at each layer : ai(x)=Wi hi-1(x)+bi\n",
    "\n",
    "        Activations (A): \n",
    "        After applying the activation function : hi(x) = g(ai(x)) \n",
    "        \"\"\"\n",
    "        cache = {\"A\": [], \"Z\": []}\n",
    "        A = X\n",
    "        cache[\"A\"].append(A)  # A0 = input\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            \"\"\"\n",
    "            Z[i]=A[i-1]W[i]+b[i]\n",
    "            Z[i] is the linear transformation (input to activation) at layer i,\n",
    "            A[i-1] is the output of the previous layer,\n",
    "            W and be are weights and biases for layer i.\n",
    "            \"\"\"\n",
    "            Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
    "            cache[\"Z\"].append(Z)\n",
    "            A = self.activation(Z)\n",
    "            cache[\"A\"].append(A)\n",
    "        \n",
    "        # Output layer with softmax activation function\n",
    "        Z = np.dot(A, self.weights[-1]) + self.biases[-1]\n",
    "        cache[\"Z\"].append(Z)\n",
    "        A = softmax(Z)\n",
    "        cache[\"A\"].append(A)\n",
    "        return A, cache\n",
    "\n",
    "    def cross_entropy_loss(self, Y_pred, Y_true):\n",
    "        \"\"\"\n",
    "        Computes cross-entropy loss.\n",
    "        \n",
    "        m is the number of examples,\n",
    "        Y_true is the one-hot encoded vector of true labels for the i-th example,\n",
    "        Y_pred is the predicted probability distribution of the i-th example.\n",
    "        \"\"\"\n",
    "        m = Y_true.shape[0]\n",
    "        loss = -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m\n",
    "        return loss\n",
    "    \n",
    "    def compute_accuracy(self, Y_pred, Y_true):\n",
    "        # Computes the accuracy of the model.\n",
    "\n",
    "        predictions = np.argmax(Y_pred, axis=1)\n",
    "        labels = np.argmax(Y_true, axis=1)\n",
    "        return np.mean(predictions == labels)\n",
    "\n",
    "\n",
    "    def backward(self, X, Y_true, cache):\n",
    "        \"\"\"\n",
    "        Performs backpropagation to compute gradients for weights and biases.\n",
    "        Returns lists of gradients for weights and biases.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        L = len(self.weights)  # Total number of layers\n",
    "        grads_W = [None] * L\n",
    "        grads_b = [None] * L\n",
    "\n",
    "        # Output layer gradient\n",
    "        A_final = cache[\"A\"][-1]\n",
    "        dZ = A_final - Y_true  # derivative for softmax + cross entropy (output layer)\n",
    "        grads_W[L - 1] = np.dot(cache[\"A\"][-2].T, dZ) / m # dW = A_prev * dZ/m (gradient of weights)\n",
    "        grads_b[L - 1] = np.sum(dZ, axis=0, keepdims=True) / m # db = dZ/m (gradient of biases)\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(L-2, -1, -1):\n",
    "            dA = np.dot(dZ, self.weights[i + 1].T)\n",
    "            Z = cache[\"Z\"][i]\n",
    "            dZ = dA * self.activation_derivative(Z)\n",
    "            grads_W[i] = np.dot(cache[\"A\"][i].T, dZ) / m\n",
    "            grads_b[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "        return grads_W, grads_b\n",
    "\n",
    "    def update_parameters(self, grads_W, grads_b, optimizer):\n",
    "        \"\"\"\n",
    "        Combines weights and biases into a single list, updates them using the provided optimizer,\n",
    "        and then splits them back into weights and biases.\n",
    "\n",
    "        W[i]=W[i]-η⋅∂Loss/∂W[i]\n",
    "        \"\"\"\n",
    "        # Combine parameters and gradients into one list each.\n",
    "        params = self.weights + self.biases\n",
    "        grads = grads_W + grads_b\n",
    "\n",
    "        # Use the optimizer to update the parameters in place.\n",
    "        updated_params = optimizer.update(params, grads)\n",
    "\n",
    "        # Split updated_params back into weights and biases.\n",
    "        L = len(self.weights)\n",
    "        self.weights = updated_params[:L]\n",
    "        self.biases = updated_params[L:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    Base Optimizer class that outlines the interface for optimization algorithms.\n",
    "    params (list): List of model parameters (weights and biases).\n",
    "    grads (list): List of gradients with respect to the model parameters.\n",
    "    \"\"\"\n",
    "    def update(self, params, grads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD)\n",
    "    Formula:\n",
    "        θ = θ - η * ∇θ\n",
    "    where, \n",
    "        θ represents the model parameters (weights/biases),\n",
    "        η is the learning rate, and \n",
    "        ∇θ is the gradient of the loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "        return params\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "    \"\"\"\n",
    "    Momentum optimizer class that helps accelerate SGD by considering past (history) gradients.\n",
    "    Formula:\n",
    "        v_t = β * v_(t-1) + ∇wt\n",
    "    where,\n",
    "        v_t is the velocity, \n",
    "        β is the momentum term, and \n",
    "        η is the learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]\n",
    "        return params\n",
    "\n",
    "class NAGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Nesterov Accelerated Gradient (NAG), improved version of momentum.\n",
    "    Formula:\n",
    "        v_t = β * v_(t-1) + ∇(wt - β * v_(t-1))   \n",
    "    where, \n",
    "        v_t is the momentum term, \n",
    "        β is the momentum coefficient, \n",
    "        η is the learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            v_prev = self.v[i].copy()\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            # Nesterov update: use the gradient evaluated at the lookahead position\n",
    "            params[i] += -self.momentum * v_prev + (1 + self.momentum) * self.v[i]\n",
    "        return params\n",
    "\n",
    "class RMSProp(Optimizer):\n",
    "    \"\"\"\n",
    "    RMSProp - Root Mean Square Propagation, which adjusts the learning rate based on recent gradients' magnitude.\n",
    "    Formula:\n",
    "        S_t = β * S_(t-1) + (1 - β) * (∇wt)^2\n",
    "        wt+1 = wt - η * ∇wt / sqrt(S_t + ε)\n",
    "    where, \n",
    "        S_t is the moving average of squared gradients, \n",
    "        β is the decay term,\n",
    "        η is the learning rate, and     \n",
    "        ε is a small number to prevent division by zero.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.S = None\n",
    "    def update(self, params, grads):\n",
    "        if self.S is None:\n",
    "            self.S = [np.zeros_like(p) for p in params]\n",
    "        for i in range(len(params)):\n",
    "            self.S[i] = self.beta * self.S[i] + (1 - self.beta) * (grads[i] ** 2)\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.S[i]) + self.epsilon)\n",
    "        return params\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    Adam optimizer, which combines momentum and RMSProp to adapt learning rates for each parameter.\n",
    "\n",
    "    Formula:\n",
    "        m_t = β1 * m_(t-1) + (1 - β1) * ∇wt\n",
    "        v_t = β2 * v_(t-1) + (1 - β2) * (∇wt))^2\n",
    "        m_t_hat = m_t / (1 - β1^t)\n",
    "        v_t_hat = v_t / (1 - β2^t)\n",
    "        wt = wt - η * m_t_hat / (sqrt(v_t_hat) + ε)\n",
    "    where,\n",
    "        m_t and v_t are moment estimates, \n",
    "        β1 and β2 are exponential decay rates, and \n",
    "        t is the time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        self.t += 1\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads[i] ** 2)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            params[i] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return params\n",
    "\n",
    "class Nadam(Optimizer):\n",
    "    \"\"\"\n",
    "    Nadam (Nesterov-accelerated Adaptive Moment Estimation),\n",
    "\n",
    "    Formula:\n",
    "        m_t+1 = β1 * m_t + (1 - β1) * ∇wt\n",
    "        v_t+1 = β2 * v_t + (1 - β2) * (∇wt)^2\n",
    "        m_t_hat = m_t+1 / (1 - β1^t+1)\n",
    "        v_t_hat = v_t+1 / (1 - β2^t+1)\n",
    "        wt+1 = wt - (η / (sqrt(v_t_hat) + ε)* (β1 * m_t+1_hat + (1 - β1) * ∇wt/ 1 - β1_t+1)\n",
    "    where, \n",
    "        β1 and β2 are exponential decay rates, and\n",
    "        t is the time step.\n",
    "    \"\"\"    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        self.t += 1\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads[i] ** 2)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            # Nadam uses a Nesterov-like momentum term:\n",
    "            nesterov_m = (self.beta1 * m_hat) + ((1 - self.beta1) * grads[i] / (1 - self.beta1 ** self.t))\n",
    "            params[i] -= self.lr * nesterov_m / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Initializing a matrix of zeros with shape (number of samples, number of classes)\n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_optimizer(name):\n",
    "    if name == \"sgd\":\n",
    "        return SGD(learning_rate=wandb.config.learning_rate)\n",
    "    elif name == \"momentum\":\n",
    "        momentum = wandb.config.get('momentum', 0.9)  # Default to 0.9 if not in config\n",
    "        return Momentum(learning_rate=wandb.config.learning_rate, momentum=momentum)\n",
    "    elif name == \"nagd\":\n",
    "        momentum = wandb.config.get('momentum', 0.9)  # Default to 0.9 if not in config\n",
    "        return NAGD(learning_rate=wandb.config.learning_rate, momentum=momentum)\n",
    "    elif name == \"rmsprop\":\n",
    "        beta = wandb.config.get('beta', 0.9)  # Default to 0.9 if not in config\n",
    "        epsilon = wandb.config.get('epsilon', 1e-8)  # Default to 1e-8 if not in config\n",
    "        return RMSProp(learning_rate=wandb.config.learning_rate, beta=beta, epsilon=epsilon)\n",
    "    elif name == \"adam\":\n",
    "        beta1 = wandb.config.get('beta1', 0.9)  # Default to 0.9 if not in config\n",
    "        beta2 = wandb.config.get('beta2', 0.999)  # Default to 0.999 if not in config\n",
    "        epsilon = wandb.config.get('epsilon', 1e-8)  # Default to 1e-8 if not i\n",
    "        return Adam(learning_rate=wandb.config.learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "    elif name == \"nadam\":\n",
    "        beta1 = wandb.config.get('beta1', 0.9)  # Default to 0.9 if not in config\n",
    "        beta2 = wandb.config.get('beta2', 0.999)  # Default to 0.999 if not in config\n",
    "        epsilon = wandb.config.get('epsilon', 1e-8)  # Default to 1e-8 if not i\n",
    "        return Nadam(learning_rate=wandb.config.learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, X_train, Y_train, epochs, batch_size):\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the training data to ensure randomness and avoid overfitting\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X_train[permutation]\n",
    "        Y_shuffled = Y_train[permutation]\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            Y_batch = Y_shuffled[start:end]\n",
    "            Y_pred, cache = model.forward(X_batch)\n",
    "            loss = model.cross_entropy_loss(Y_pred, Y_batch)\n",
    "            accuracy = model.compute_accuracy(Y_pred, Y_batch)\n",
    "            epoch_loss += loss\n",
    "            grads_W, grads_b = model.backward(X_batch, Y_batch, cache)\n",
    "            model.update_parameters(grads_W, grads_b, optimizer)  # Updated model parameters (weights and biases) using the optimizer\n",
    "\n",
    "        # Calculate the average loss for the epoch (mean loss over all batches)\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy}\")\n",
    "        wandb.log({\"epoch\": epoch+1, \"loss\": avg_loss, \"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    wandb.init(\n",
    "        project=projectId, \n",
    "        config={\n",
    "            \"input_size\": 784,\n",
    "            \"hidden_layers\": [128, 64, 32],\n",
    "            \"output_size\": 10,\n",
    "            \"activation\": \"relu\",     \n",
    "            \"weight_init\": \"random\",    \n",
    "            \"learning_rate\": 0.001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"beta\": 0.9,               \n",
    "            \"beta1\": 0.9,              \n",
    "            \"beta2\": 0.999,            \n",
    "            \"epsilon\": 1e-8,\n",
    "            \"optimizer\": \"adam\",   \n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 5\n",
    "        })\n",
    "    \n",
    "    config = wandb.config\n",
    "    samples = 5\n",
    "\n",
    "    # Load and preprocessing Fashion-MNIST data\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "    Y_train = one_hot_encode(y_train, config.output_size)\n",
    "    Y_test = one_hot_encode(y_test, config.output_size)\n",
    "\n",
    "    # Initializing model and optimizer\n",
    "    model = FeedForwardNN(config.input_size, config.hidden_layers, config.output_size,\n",
    "                          activation=config.activation, weight_init=config.weight_init)\n",
    "    optimizer = get_optimizer(config.optimizer)\n",
    "\n",
    "    # Training the model\n",
    "    train(model, optimizer, X_train, Y_train, epochs=config.epochs, batch_size=config.batch_size)\n",
    "\n",
    "    # After training, evaluating on a few samples and log results\n",
    "    Y_pred, _ = model.forward(X_train[:samples])\n",
    "    for i in range(Y_pred[:samples].shape[0]):\n",
    "        print(\"Probability distribution over 10 classes of a single sample:\", Y_pred[i])\n",
    "    predictions = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "    Y_test_pred = model.forward(X_test)\n",
    "    predictions_test = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "    print(\"Predictions for first 5 training samples:\", predictions)\n",
    "    print(\"Original labels for test data:\", Y_test)\n",
    "    \n",
    "    # Loging table of predictions\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    table = wandb.Table(columns=[\"Sample\", \"Predicted Class\", \"Class Name\"])\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        table.add_data(i, pred, class_names[pred])\n",
    "    wandb.log({\"Predictions\": table})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify the predictions for samples\n",
    "true_classes = np.argmax(Y_train[:samples], axis=1)  # True class indices\n",
    "\n",
    "fig, axes = plt.subplots(1, samples, figsize=(15, 9))  # 1 rows, 5 columns\n",
    "axes = axes.flatten()  # Flatten axes array for easy indexing\n",
    "\n",
    "for i in range(samples):\n",
    "    # Plot the image\n",
    "    ax = axes[i]\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap=\"gray\")  # Reshaping the flattened image to 28x28\n",
    "\n",
    "    # Title showing prediction result\n",
    "    pred_class = predictions[i]\n",
    "    true_class = true_classes[i]\n",
    "    pred_class_name = class_names[pred_class]\n",
    "    true_class_name = class_names[true_class]\n",
    "    \n",
    "    # Check if the prediction is correct or not\n",
    "    correct = 'Correct' if pred_class == true_class else 'Incorrect'\n",
    "\n",
    "    ax.set_title(f\"Pred: {pred_class_name}\\nTrue: {true_class_name}\\n{correct}\")\n",
    "    ax.axis('off')  # Hide axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "table = wandb.Table(columns=[\"Sample\", \"Predicted Class\", \"True Class\", \"Correct?\"])\n",
    "for i in range(samples):\n",
    "    pred_class = predictions[i]\n",
    "    true_class = true_classes[i]\n",
    "    pred_class_name = class_names[pred_class]\n",
    "    true_class_name = class_names[true_class]\n",
    "    correct = 'Correct' if pred_class == true_class else 'Incorrect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    # Initialize wandb for the current run. The hyperparameters come from wandb.config.\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "    run_name = f\"hl_{config.hidden_layers}_bs_{config.batch_size}_ac_{config.activation.lower()}\"\n",
    "    print(run_name)\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    input_size = config.input_size\n",
    "    output_size = config.output_size\n",
    "    hidden_layers = config.hidden_layers  # Number of hidden layers\n",
    "    hidden_size = config.hidden_size  # Size of each hidden layer\n",
    "    activation_function = config.activation  # Activation function\n",
    "    weight_init = config.weight_init  # Weight initialization method\n",
    "    optimizer_name = config.optimizer  # Optimizer choice\n",
    "    batch_size = config.batch_size  # Batch size\n",
    "    num_epochs = config.epochs  # Number of epochs\n",
    "    learning_rate = config.learning_rate  # Learning rate\n",
    "    weight_decay = config.weight_decay  # Weight decay (L2 regularization)\n",
    "\n",
    "    \n",
    "    # Load Fashion-MNIST data (using the standard train/test split)\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "    # Preprocess the images: flatten and normalize to [0, 1]\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "    # Create a validation split: 90% training, 10% validation\n",
    "    num_train = int(0.9 * X_train.shape[0])\n",
    "    permutation = np.random.permutation(X_train.shape[0])\n",
    "    X_train = X_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "    X_train_split = X_train[:num_train]\n",
    "    X_val_split = X_train[num_train:]\n",
    "    y_train_split = y_train[:num_train]\n",
    "    y_val_split = y_train[num_train:]\n",
    "\n",
    "    # Use the local variable \"output_size\" instead of config.output_size\n",
    "    num_classes = output_size  # typically 10\n",
    "    Y_train_split = one_hot_encode(y_train_split, num_classes)\n",
    "    Y_val_split = one_hot_encode(y_val_split, num_classes)\n",
    "    hidden_layers = [hidden_size] * hidden_layers  # This will create a list of hidden layers\n",
    "\n",
    "    # Instantiate the model (FeedForwardNN is assumed to be defined with backprop as in Question 3)\n",
    "    model = FeedForwardNN(\n",
    "        input_size=input_size,\n",
    "        output_size=output_size,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation=activation_function.lower(),  # convert to lowercase if needed\n",
    "        weight_init=weight_init.lower()\n",
    "    )\n",
    "    optimizer = get_optimizer(optimizer_name)\n",
    "    num_samples = X_train_split.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    training_loss = None\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data at each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_train_epoch = X_train_split[permutation]\n",
    "        Y_train_epoch = Y_train_split[permutation]\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_train_epoch[start:end]\n",
    "            Y_batch = Y_train_epoch[start:end]\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred, cache = model.forward(X_batch)\n",
    "            loss = model.cross_entropy_loss(Y_pred, Y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            grads_W, grads_b = model.backward(X_batch, Y_batch, cache)\n",
    "\n",
    "            # Update parameters using the chosen optimizer\n",
    "            model.update_parameters(grads_W, grads_b, optimizer)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches   # training_loss = avg_loss\n",
    "\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        Y_val_pred, _ = model.forward(X_val_split)\n",
    "        val_loss = model.cross_entropy_loss(Y_val_pred, Y_val_split)\n",
    "\n",
    "        predictions = np.argmax(Y_val_pred, axis=1)\n",
    "        correct = np.sum(predictions == y_val_split)\n",
    "        val_accuracy_epoch = correct / len(y_val_split)\n",
    "\n",
    "\n",
    "        # Log epoch-level metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"loss\": avg_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy_epoch,\n",
    "        })\n",
    "\n",
    "    print(f\"Sweep run complete: Best Epoch: {best_epoch}, Best Val Loss: {best_val_loss:.4f}, Val Accuracy: {val_accuracy_epoch:.4f}\")\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  #(bayes) Using random search for efficiency in high-dimensional spaces\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {'values': [5, 10]},\n",
    "        'hidden_layers': {'values': [3, 4, 5]},  # Number of hidden layers\n",
    "        'hidden_size': {'values': [32, 64, 128]},  # Size of each hidden layer\n",
    "        'weight_decay': {'values': [0, 0.0005, 0.5]},  # L2 regularization factor\n",
    "        'learning_rate': {'values': [0.0001, 0.001, 0.01]},\n",
    "        'optimizer': {'values': ['sgd', 'momentum', 'nagd', 'rmsprop', 'adam', 'nadam']},\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'weight_init': {'values': ['random', 'xavier']},\n",
    "        'activation': {'values': ['sigmoid', 'tanh', 'relu']},  # Use lowercase for consistency\n",
    "        'input_size': {'value': 784},  # Fixed input size for Fashion-MNIST\n",
    "        'output_size': {'value': 10},   # Fixed output classes\n",
    "        'epsilon':{ 'values': [1e-8, 1e-7, 1e-6]},\n",
    "        'beta': {'values': [0.9, 0.99, 0.999]},  # Possible values for beta\n",
    "        'momentum': {'values': [0.8, 0.9, 0.95]},  # Possible values for momentum\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project=projectId)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wandb.agent(sweep_id, function=train_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
